{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Data from Twitter\n",
    "The following block will use Selenium to scrape tweets from twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infinity_scroll(driver,filename):\n",
    "    try:\n",
    "        for i in range(1000):\n",
    "            \n",
    "            soupObj = BeautifulSoup(driver.page_source,'html.parser')\n",
    "            \n",
    "            with open(filename,'a') as f: #remember to delete the file everytime you run it.\n",
    "                f.write(str(soupObj))\n",
    "            \n",
    "            driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "            time.sleep(2)\n",
    "\n",
    "    except:\n",
    "        return\n",
    "        \n",
    "    return\n",
    "\n",
    "\n",
    "def scrape_tweets(twitter_handle,from_date,to_date,filename):\n",
    "    \n",
    "    url = f'https://twitter.com/search?q=(from%3A{twitter_handle})%20until%3A{to_date}%20since%3A{from_date}%20-filter%3Alinks%20-filter%3Areplies&src=typed_query&f=live'\n",
    "    response = requests.get(url)\n",
    "    driver = webdriver.Firefox()\n",
    "    driver.get(url)\n",
    "    time.sleep(10) \n",
    "    \n",
    "    # scrolls down to the bottom of the page\n",
    "    infinity_scroll(driver,filename)\n",
    "\n",
    "    driver.quit()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_handle = 'gsk'\n",
    "# twitter_handle = 'sanofi'\n",
    "# twitter handle = 'AstraZeneca'\n",
    "twitter_handle = 'realdonaldtrump'\n",
    "\n",
    "# twitter_handle = 'elonmusk'\n",
    "from_date = '2020-01-01'\n",
    "to_date = '2020-09-24'\n",
    "\n",
    "\n",
    "scrape_tweets(twitter_handle,from_date,to_date,'trump_tweets_Jan_2020_Sept_2020.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Tweet Data\n",
    "The following code processes the data pulled from the XML file to create a listing of all Tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets2(filename):\n",
    "    with open(filename,'r') as f:\n",
    "        soupObj = BeautifulSoup(f,'html.parser')\n",
    "    \n",
    "    total_tweets = len(soupObj.findAll(class_=\"css-1dbjc4n r-1iusvr4 r-16y2uox r-1777fci r-1mi0q7o\"))\n",
    "    tweet_listing = []\n",
    "    time_listing = []\n",
    "    \n",
    "    tweet_block = soupObj.find(class_=\"css-1dbjc4n r-1iusvr4 r-16y2uox r-1777fci r-1mi0q7o\")\n",
    "    \n",
    "    for i in range(total_tweets):\n",
    "        try:\n",
    "            \n",
    "            tweet = tweet_block.find('div',\"css-901oao r-jwli3a r-1qd0xha r-a023e6 r-16dba41 r-ad9z0x r-bcqeeo r-bnwqim r-qvutc0\")\n",
    "            tweet_listing.append(tweet.text)\n",
    "        \n",
    "        except AttributeError:\n",
    "            tweet_listing.append(np.NaN)\n",
    "        \n",
    "        try:\n",
    "            timestamp = tweet_block.find('time')\n",
    "            time_listing.append((timestamp.find_next('time').attrs)['datetime'])\n",
    "        \n",
    "        except AttributeError:\n",
    "            time_listing.append(np.NaN)\n",
    "\n",
    "        tweet_block = tweet_block.find_next(class_=\"css-1dbjc4n r-1iusvr4 r-16y2uox r-1777fci r-1mi0q7o\")\n",
    "\n",
    "    return pd.DataFrame({'Time':time_listing,'Tweet':tweet_listing})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath = 'astrazeneca_tweets_Oct_2017_Sept_24_2020.xml'\n",
    "# filepath = 'gsk_tweets_Oct_2017_Sept_24_2020.xml'\n",
    "# filepath = 'pfizer_tweets_Oct_2017_Sept_24_2020.xml'\n",
    "# filepath = 'elon_tweets_Sept_2019_Sept_2020.xml'\n",
    "filepath = 'trump_tweets_Jan_2020_Sept_2020.xml'\n",
    "\n",
    "df = get_tweets2(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MANUAL ADJUSTMENT\n",
    "## Not sure why but all the tweets scraped are offset by 1 with the first date value missing.\n",
    "## In the interest of time, I'm shifting the cells and manually adding it back:\n",
    "\n",
    "df_copy['Time'] = df_copy[['Time']].shift()\n",
    "df_copy.loc[[0,0],'Time'] = \"2020-09-23T14:03:08.000Z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-09-23T14:03:08.000Z</td>\n",
       "      <td>Very important that, in order to watch that AL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-09-23T13:58:47.000Z</td>\n",
       "      <td>Bay of Pigs Veterans &amp; Hispanic Heritage Remar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-09-23T13:21:10.000Z</td>\n",
       "      <td>White House News Conference today at 6:00 P.M....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-09-23T11:22:58.000Z</td>\n",
       "      <td>I hardly know Cindy McCain other than having p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-09-22T22:53:22.000Z</td>\n",
       "      <td>A few weeks ago, I BANNED efforts to indoctrin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Time                                              Tweet\n",
       "0  2020-09-23T14:03:08.000Z  Very important that, in order to watch that AL...\n",
       "1  2020-09-23T13:58:47.000Z  Bay of Pigs Veterans & Hispanic Heritage Remar...\n",
       "2  2020-09-23T13:21:10.000Z  White House News Conference today at 6:00 P.M....\n",
       "3  2020-09-23T11:22:58.000Z  I hardly know Cindy McCain other than having p...\n",
       "4  2020-09-22T22:53:22.000Z  A few weeks ago, I BANNED efforts to indoctrin..."
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('gsk_tweets_Oct_2017_Sept_24_2020.csv')\n",
    "# df.to_csv('pfizer_tweets_Oct_2017_Sept_24_2020.csv')\n",
    "# df.to_csv('astrazeneca_tweets_Oct_2017_Sept_24_2020.csv')\n",
    "# df.to_csv('elon_tweets_Sept_2019_Sept_2020.csv')\n",
    "\n",
    "df_copy.to_csv('trump_tweets_Jan_2020_Sep_2020.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------\n",
    "# Code Graveyard\n",
    "Code that is not currently in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to extract tweets from xml file, however it was found not to work in tweets that were censored\n",
    "# replaced by get_tweets2\n",
    "def get_tweets(filename):\n",
    "\n",
    "    with open(filename,'r') as f:\n",
    "        soupObj = BeautifulSoup(f,'html.parser')\n",
    "           \n",
    "    num_tweets = len(soupObj.find_all('div',\"css-901oao r-jwli3a r-1qd0xha r-a023e6 r-16dba41 r-ad9z0x r-bcqeeo r-bnwqim r-qvutc0\"))\n",
    "    num_times = len(soupObj.find_all('time'))\n",
    "    \n",
    "    timestamp = soupObj.find('time')\n",
    "    tweet = soupObj.find('div',\"css-901oao r-jwli3a r-1qd0xha r-a023e6 r-16dba41 r-ad9z0x r-bcqeeo r-bnwqim r-qvutc0\")\n",
    "    \n",
    "    tweet_listing = [tweet.text]\n",
    "    time_listing = [timestamp.attrs['datetime']]\n",
    "    \n",
    "    for i in range(max(num_tweets,num_times)):\n",
    "        try:\n",
    "            #pull timestamps\n",
    "            time_listing.append((timestamp.find_next('time').attrs)['datetime'])\n",
    "            timestamp = timestamp.find_next('time')\n",
    "            \n",
    "        except AttributeError:\n",
    "            time_listing.append(np.NaN)\n",
    "        try:\n",
    "            # pull tweets\n",
    "            tweet_listing.append(tweet.find_next('div',\"css-901oao r-jwli3a r-1qd0xha r-a023e6 r-16dba41 r-ad9z0x r-bcqeeo r-bnwqim r-qvutc0\").text)\n",
    "            tweet = tweet.find_next('div',\"css-901oao r-jwli3a r-1qd0xha r-a023e6 r-16dba41 r-ad9z0x r-bcqeeo r-bnwqim r-qvutc0\")\n",
    "        \n",
    "        except AttributeError:\n",
    "            tweet_listing.append(np.NaN)\n",
    "\n",
    "\n",
    "    return pd.DataFrame({'Time':time_listing,'Tweet':tweet_listing})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GetOldTweets3 no longer works since twitter changed the endpoint. Only been an issue for 4 days, but may be fixed.\n",
    "# save code incase the developers fix it.\n",
    "# import GetOldTweets3 as got\n",
    "# tweetCriteria = got.manager.TweetCriteria().setQuerySearch(\"realDonaldTrump\").setMaxTweets(2)\n",
    "# tweet = got.manager.TweetManager.getTweets(tweetCriteria)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code used in testing\n",
    "# variables for testing scrape_tweets()\n",
    "# 'https://twitter.com/search?q=(from%3Arealdonaldtrump)%20until%3A2020-07-01%20since%3A2020-01-01%20-filter%3Alinks%20-filter%3Areplies&src=typed_query&f=live'\n",
    "# twitter_handle = 'realdonaldtrump'\n",
    "# from_date = '2020-01-01'\n",
    "# to_date = '2020-07-01'\n",
    "\n",
    "# other scrapers:\n",
    "# snscrape's jsonl\n",
    "# https://github.com/twintproject/twint/issues/918#issuecomment-696448934"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
